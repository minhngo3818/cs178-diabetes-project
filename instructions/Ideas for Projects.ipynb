{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52abf41b-038c-4469-980f-0a79acd81c62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ideas for Projects\n",
    "\n",
    "To write a good project report you should to go beyond just reporting accuracy numbers for your classifiers. The goal of the projects is for you to learn some in depth knowledge and gain some insights about the machine learning classifiers you are working with, by exploring these methods in different ways\n",
    "\n",
    "Below are some suggestions for ideas that you can add to your project to make it more interesting: you can focus on one of these ideas, or combinations of them, or you may have your own ideas for exploring your classifiers that are not listed here (and that is fine). \n",
    "\n",
    "**Evaluating Classifiers:**\n",
    " - do a \"deeper dive\" to compare models using more than just accuracy, e.g., confusion matrices, rank-based metrics like AUC, precision, recall\n",
    " - sklearn has a good tutorialLinks to an external site. with code on different ways to evaluate classification models, to provide you with some ideas\n",
    " - you could also analyze what types of errors a classifier makes, e.g., are the errors it makes the same sort of errors a human might make (for image and text data in particular)\n",
    " - do the different classifiers you trained tend to make errors on the same examples? or do they make systematically different errors?\n",
    "\n",
    "**Learning Curves:**\n",
    " - compute accuracy/error as a function of training set size (e.g., by creating smaller data sets using random subsamples of your data: for small amounts of training data you may want to repeat this and average over multiple small subsets to get a reliable number)\n",
    " - differences between classifiers can change as the dataset size changes\n",
    "  - sklearn even has a function to help you create learning curves:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curveLinks to an external site.\n",
    "\n",
    "**Error versus Complexity Tradeoffs**\n",
    " - see if you can generate the type of train/test error versus complexity graphs we discussed in lectures\n",
    " - note that you may need to vary the training set size (e.g., make it smaller than the default size) in order to see significant overfitting\n",
    "\n",
    "**Interpreting and Understanding Classifiers**\n",
    "  - see the pages on \"inspection\" in sklearnLinks to an external site. for some general suggestions \n",
    "  - for neural networks see methods like LIMELinks to an external site. (co-invented by Prof Sameer Singh, a professor in the CS dept at UCI)\n",
    "  - for visualizing convolutional filters, some ideas are presented hereLinks to an external site. \n",
    "  - for datasets with not too many features (e.g., the diabetes data) you can experiment with removing one feature at a time from your training set, training and testing the model each time, to see what the effect is on accuracy of removing each feature from the model\n",
    "\n",
    "**Classifier Calibration**\n",
    " - are the probabilities being produced by your model \"calibrated?\"\n",
    " - sklearn has a tutorial introduction to calibration with codeLinks to an external site. \n",
    " - for many applications, having well-calibrated probabilities from a classifier is important\n",
    "\n",
    "**Ensembles**\n",
    "\n",
    "You could experiment with some of the ensembling techniques we discussed in class (voting, averaging, stacking) to see if a combination of your classifiers is more accurate than any single one\n",
    "\n",
    "**Prediction Speed of a Trained Classifier**\n",
    "  - how fast are each of your trained models at making predictions? for real-world applications this can be important\n",
    "  - you can give your model (say) 10,000 or more test examples, measure the total prediction time, and divide by 1000 to get the prediction time (say in milliseconds) per example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2740b-e496-40bd-a8ac-90ae9c261f89",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
